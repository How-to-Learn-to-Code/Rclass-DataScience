---
title: "Working with real-world data"
subtitle: "H2L2C Project Management Day 2"
author: "Lorrie He, Matthew Sutcliffe"
format: html
output:
  html_document:
editor: visual
---

# Final Class!

Now that you've been through the entire course, we want to reinforce some of the skills that you've learned by applying your knowledge to several existing datasets and have you go through the wrangling, analysis, and visualization process as much as you can on your own. By the end of this lesson, you should have ... //TODO \[some code that shows...\]

For each of these datasets (as many as time allows), we'd like you to do some basic analysis, including the following steps:

1.  Create a directory for the analysis of this dataset, using best practices discussed in the previous class.
2.  Load in the data.
3.  Perform some initial data exploration (What are the rows? What are the columns? How many samples does this dataset have? etc.)
4.  Identify at least 1 research question that you could try to answer with this dataset.
5.  Format the data in a way that allows for this analysis.
6.  Visualize the data in helpful ways to answer your question.
7.  Make your code reproducible by using Github.

To help with your analyses, we have included a **template document** outlining these steps that you are welcome to use.

//TODO more pointers for what to do for analyses, 1-2 example figs/analyses, more decripts of the data structure, so the teacher doesnt have to be super familiar with the data. crunchy fig. prob this is mostly going to be a "here's a figure - recreate it". def need to include some code to copy/paste for certain operations

# Datasets

The csvs are available under the `data` folder (or wherever it'll be moved to). We have provided links to data that is publically available for download, as well as some related publications that may give you more information about the data.

These datasets were chosen because we thought they represented a good variety in the different kinds of wrangling and visualization challenges you might encounter with your own data in the future, and are presented roughly in order of increasing experimental/wrangling complexity. We have highlighted a few specific R skills that some of these datasets were meant to challenge you on.

For each dataset, we have provided a description of the research question/data collection methods, metadata, and the first few lines of each dataset. Read through these descriptions and work on the dataset you find most interesting.

::: callout-tip
## But I thought this was HTLTCode, not HTLTScience!

Although we heavily encourage you to develop your own research questions and ideas for analyses to really practice your data analysis skills, we acknowledge that not everyone may have enough background to ask "appropriate" research questions for all of these datasets: so, to allow you to focus on the coding, for each dataset, we have come up with a few research questions of varying difficulty (i.e. step 4 from the list above) and created some example figures to accompany them.

Depending on your familiarity with the topic, you can either create your own figure to answer the research question, or use your skills to figure out how we went from raw data to our final figure!
:::

::: callout-note
## Up for a challenge?

Some of these datasets come with additional files or have even less pre-processed versions. If you're up for a challenge (or are revisiting this material and feeling more confident in your skills), we have included those csvs for your perusal.

idk if will keep this
:::

//TODO add metadata + data previews (metadata via callouts)

## Western Africa Ebola public health dataset

[Data source](https://www.kaggle.com/datasets/imdevskp/ebola-outbreak-20142016-complete-dataset)

**Background:** The Western African Ebola virus (EV) epidemic of 2013-2016 is the most severe outbreak of the EV disease in history. It caused major disruptions and loss of life, mainly in the republics of Guinea, Liberia, and Sierra Leone.

**Data overview:** The following data contains the dates for when each region updated their confirmed cases and deaths – consider how you might illustrate the dynamics of the outbreak.

## Heat exposure in Phoenix, Arizona ecological dataset

[Data source](https://data.sustainability-innovation.asu.edu/cap-portal/metadataviewer?packageid=knb-lter-cap.647.2) \| [Related publication](https://doi.org/10.1016/j.envint.2020.106271)

**Background:** Exposure to extreme heat is of growing concern with the rise of urbanization and ongoing climate change. Though most current knowledge about heat-health risks are known and implemented at the neighborhood level, less is known about individual experiences of heat, which can vary due to differences in access to cooling resources and activity patterns.

**Data overview:** The Central Arizona-Pheonix Long-Term Ecological Research Program [(CAP-LTER)](https://sustainability-innovation.asu.edu/caplter/) recruited participants from 5 Pheonix-area neighborhoods to wear air temperature sensors that recorded their individually-experienced temperatures (IETs) as they went about their daily activities – consider how you might represent the relationships between individual activity and neighborhood.

## AVIDA digital evolution dataset

[Related publications](https://avida-ed.msu.edu/digital-evolution/)[^1]

[^1]: In particular, see [Lenski et al. (2003)](https://www.nature.com/articles/nature01568) and [Smith et al. (2016)](https://link.springer.com/article/10.1186/s12052-016-0060-0).

**Background:** The following data was generated using [Avida-ED](https://avida-ed.msu.edu/avida-ed-application/), an online educational application that allows one to study the dynamics of evolutionary processes. Digital, asexually-reproducing organisms known as "Avidians" can be placed into something akin to a virtual Petri dish to evolve in, and one can manipulate parameters such as mutation rate, resource availability, and dish size to study how those factors affect the evolution of the population.

Your friend needs your help to analyze their AVIDA-Ed data. They have designed a series of experiments around a mutant Avidian that gets an energy bonus when the sugar "nanose" is present and a wildtype Avidian that does not. They wanted to see how competition and resource availability affect the population dynamics of these Avidians.

**Data overview:** Time was measured in "updates". Your friend grew either the wildtype only, mutant only, or both populations together (competition) in the following 3 environments:

-   Minimal (no additional sugars present)

-   Selective (only nanose present)

-   Rich (nanose and additional sugars present)

//TODO crunchy fig

```{r dplyr_syntax,echo=FALSE, fig.align = 'center', out.width = "80%", fig.cap = "AVIDA experiment setup"}
knitr::include_graphics("project-day-2_files/avida-expt.png")
```

They collected data on the following population metrics: (reword: got a lot of data?)

-   Avg fitness (avg individual reproductive success; ranges 0-1)

-   Avg offspring cost (avg cost to reproduce, ranges \>0)

-   Avg energy acquisition rate (avg rate of acquiring energy from the environment; ranges \>0)

-   The population size (ranges from 1-900)

-   The viable population size (might remove this one bc confusing)

Help your friend get started with some of the exploratory data analysis. You can explore how combinations of genotype, competition, resource availability affect the population metrics – while you do not need to use all of the data files, you may need to mix-and-match different components of the data depending on your research question.

*For teachers:* may need to help students with joining dataframes, parsing out column names, pivoting, and hopefully not the biology parts... once the columns are pivoted they can just slap different metrics on there. can plot time series data.

===

extra stuff

In the previous class, you completed the following steps:

1.  Created a GitHub account.

2.  Linked your GitHub account with your Longleaf account.

3.  Created and pushed your first repository to GitHub.
